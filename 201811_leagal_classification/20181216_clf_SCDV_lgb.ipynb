{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 54\n",
      "10 44\n",
      "20 75\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import MeCab\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "##### ファイルをリストに格納 #####\n",
    "texts = []\n",
    "dir_name_txt = \"non_death_txt\"\n",
    "os.chdir('data/'+dir_name_txt)\n",
    "name = glob.glob(\"*\") #ファイル名リスト\n",
    "for i in range(len(name)):\n",
    "    f = open(str(name[i]), encoding=\"utf-8\")\n",
    "    texts.append(f.read())  # ファイル終端まで全て読んだデータを返す\n",
    "    f.close()\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "print(len(name),len(texts))\n",
    "\n",
    "data = []\n",
    "num_test = 10 # testデータの数\n",
    "for i in range(len(name)):\n",
    "    ##### 名詞をリストに格納 #####\n",
    "    items = []\n",
    "    # パース\n",
    "    mecab = MeCab.Tagger()\n",
    "    parse = mecab.parse(texts[i])\n",
    "    lines = parse.split('\\n')\n",
    "    items = (re.split('[\\t,]', line) for line in lines)\n",
    "    \n",
    "\n",
    "    # 名詞をリストに格納\n",
    "    words = [item[0]\n",
    "             for item in items\n",
    "             if (item[0] not in ('EOS', '', 't', 'ー') and\n",
    "                 item[1] == '名詞' and item[2] == '一般')]\n",
    "\n",
    "    # 名詞の間にスペースを入れて1文にする\n",
    "    data_ = \"\"\n",
    "    for j in range(len(words)):\n",
    "        data_ = data_ + \" \"+words[j]\n",
    "    data.append(data_)\n",
    "    #print(len(data))\n",
    "\n",
    "hanrei_test = {\"file_name\":[],\"data\":[],\"target_name\":[],\"target\":[]}\n",
    "for i in range(num_test):\n",
    "    hanrei_test[\"file_name\"].append(name[i])\n",
    "    hanrei_test[\"data\"].append(data[i])\n",
    "    hanrei_test[\"target_name\"].append(\"non_death\")\n",
    "    hanrei_test[\"target\"].append(0)\n",
    "\n",
    "hanrei_train = {\"file_name\":[],\"data\":[],\"target_name\":[],\"target\":[]}\n",
    "for i in range(num_test,len(data)):\n",
    "    hanrei_train[\"file_name\"].append(name[i])\n",
    "    hanrei_train[\"data\"].append(data[i])\n",
    "    hanrei_train[\"target_name\"].append(\"non_death\")\n",
    "    hanrei_train[\"target\"].append(0)\n",
    "    #{\"data\":str(i), \"target_name\":str(i), \"target\":str(i)})\n",
    "print(len(hanrei_test[\"data\"]),len(hanrei_train[\"data\"]))\n",
    "\n",
    "#-----------------------死刑判例文(1)--------------------------#\n",
    "\n",
    "##### ファイルをリストに格納 #####\n",
    "texts = []\n",
    "dir_name_txt = \"death_txt\"\n",
    "os.chdir('data/'+dir_name_txt)\n",
    "name = glob.glob(\"*\")\n",
    "for i in range(len(name)):\n",
    "    f = open(str(name[i]), encoding=\"utf-8\")\n",
    "    texts.append(f.read())  # ファイル終端まで全て読んだデータを返す\n",
    "    f.close()\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "#print(len(name),len(texts))\n",
    "\n",
    "data = []\n",
    "for i in range(len(name)):\n",
    "    ##### 名詞をリストに格納 #####\n",
    "    items = []\n",
    "    # パース\n",
    "    mecab = MeCab.Tagger()\n",
    "    parse = mecab.parse(texts[i])\n",
    "    #print(parse)\n",
    "    lines = parse.split('\\n')\n",
    "    items = (re.split('[\\t,]', line) for line in lines)\n",
    "    #print(items)\n",
    "    # 名詞をリストに格納\n",
    "    words = [item[0]\n",
    "             for item in items\n",
    "             if (item[0] not in ('EOS', '', 't', 'ー') and\n",
    "                 item[1] == '名詞' and item[2] == '一般')]\n",
    "\n",
    "    #名詞の間にスペースを入れて1文にする\n",
    "    data_ = \"\"\n",
    "    for j in range(len(words)):\n",
    "        data_ = data_ + \" \"+words[j]\n",
    "        \n",
    "    data.append(data_)\n",
    "#print(data)\n",
    "\n",
    "#print(len(data))\n",
    "for i in range(num_test):\n",
    "    hanrei_test[\"file_name\"].append(name[i])\n",
    "    hanrei_test[\"data\"].append(data[i])\n",
    "    hanrei_test[\"target_name\"].append(\"death\")\n",
    "    hanrei_test[\"target\"].append(1)\n",
    "\n",
    "for i in range(num_test,len(data)):\n",
    "    hanrei_train[\"file_name\"].append(name[i])\n",
    "    hanrei_train[\"data\"].append(data[i])\n",
    "    hanrei_train[\"target_name\"].append(\"death\")\n",
    "    hanrei_train[\"target\"].append(1)\n",
    "    \n",
    "\n",
    "print(len(hanrei_test[\"data\"]),len(hanrei_train[\"data\"]))\n",
    "#print(hanrei_train[\"target_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create word2vec\n",
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import MeCab\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "import re\n",
    "\n",
    "start = time.time()\n",
    "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
    "sentences = []\n",
    "print (\"Parsing sentences from training set...\")\n",
    "\n",
    "# Loop over each news article.\n",
    "for review in tqdm(df[\"news\"]):\n",
    "    try:\n",
    "        # Split a review into parsed sentences.\n",
    "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "        h = result.split(\" \")\n",
    "        h = list(filter((\"\").__ne__, h))\n",
    "        sentences.append(h)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = 200     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "print (\"Training Word2Vec model...\")\n",
    "# Train Word2Vec model.\n",
    "model = Word2Vec(sentences, workers=num_workers, hs = 0, sg = 1, negative = 10, iter = 25,\\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=1)\n",
    "\n",
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "model.init_sims(replace=True)\n",
    "# Save Word2Vec model.\n",
    "print (\"Saving Word2Vec model...\")\n",
    "model.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+model_name)\n",
    "endmodeltime = time.time()\n",
    "\n",
    "print (\"time : \", endmodeltime-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
